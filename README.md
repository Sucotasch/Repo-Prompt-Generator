# Repo-Prompt-Generator Technical Documentation fully generated by Repo-Prompt-Generator itself!

## 1. Overview
**Repo-Prompt-Generator** is a sophisticated developer tool designed to transform entire software repositories into highly structured, context-aware prompts for Large Language Models (LLMs) like Google Gemini or local models via Ollama. 

Unlike simple "copy-paste" tools, this application implements **Retrieval-Augmented Generation (RAG)** and **automated summarization** to handle large codebases that would otherwise exceed LLM context limits. It allows developers to "talk" to their code by generating a comprehensive system prompt that includes architecture, dependencies, and relevant source code.

### Real Capabilities
*   **Dual Input Modes:** Analyze public GitHub repositories via URL or local directories via the browser's File System API.
*   **RAG-Powered Context Filtering:** Uses local embeddings to select only the most relevant code chunks based on a specific user query.
*   **Local LLM Integration (Ollama):** Perform summarization, embeddings, and final prompt generation locally to preserve privacy or reduce API costs.
*   **Intelligent Summarization:** Automatically condenses long files into key logic summaries using Ollama before sending them to the final LLM.
*   **Template System:** Pre-defined instructions for common tasks (e.g., "Code Review," "Refactoring," "Documentation Generation").
*   **Context Management:** Manual controls for Max Files, Context Window (tokens), and Top-K RAG segments.

---

## 2. Architecture and Algorithm of Operation

### High-Level Architecture
The application is built as a modern React SPA (Single Page Application) with a lightweight Express backend for serving. The architecture is service-oriented:

1.  **UI Layer (React/Tailwind):** Manages state, configuration, and file selection.
2.  **Github/Local Services:** Responsible for fetching raw file content and metadata.
3.  **RAG Service:** Handles text chunking and vector similarity search.
4.  **AI Services (Gemini/Ollama):** Interfaces for generating embeddings and text.

### Logic Flow (The Algorithm)
The generation process follows these sequential steps:

1.  **Ingestion:** 
    *   Files are pulled from GitHub or read from a local folder.
    *   The system filters out boilerplate and non-source files (images, binaries).
2.  **Preprocessing (Optional Summarization):** 
    *   If enabled, each file is sent to a local Ollama instance. 
    *   Ollama generates a "logic summary" to reduce the token count while retaining architectural intent.
3.  **RAG Loop (Optional Search):**
    *   The "Additional Context" provided by the user is converted into a vector embedding.
    *   The codebase is split into 30-line chunks.
    *   The system calculates the cosine similarity between the query embedding and the code chunks.
    *   Only the **Top-K** most relevant chunks are kept.
4.  **Prompt Assembly:** 
    *   A system prompt is constructed combining: Repository Metadata + README + Dependencies + Filtered/Summarized Source Code + Custom User Instructions.
5.  **Final Generation:**
    *   The assembled text is sent to either **Google Gemini API** or a **Local Ollama model** to produce the final `.md` prompt.

---

## 3. Installation and Configuration

### Prerequisites
*   **Node.js** (v18 or higher recommended)
*   **Gemini API Key** (from Google AI Studio)
*   **Ollama** (Optional, for local processing features)

### Setup Steps
1.  **Clone and Install:**
    ```bash
    git clone https://github.com/Sucotasch/Repo-Prompt-Generator.git
    cd Repo-Prompt-Generator
    npm install
    ```

2.  **Environment Variables:**
    Create a `.env.local` file in the root directory:
    ```env
    VITE_GEMINI_API_KEY=your_api_key_here
    ```

3.  **Configure Ollama (Crucial for Local Features):**
    To allow the web browser to communicate with your local Ollama instance, you must enable CORS.
    *   **Windows:** The app provides a "Download .bat" feature that sets `OLLAMA_ORIGINS` and restarts the service.
    *   **Manual (Linux/Mac):**
        ```bash
        export OLLAMA_ORIGINS="http://localhost:5173,http://127.0.0.1:5173"
        ollama serve
        ```

4.  **Launch:**
    ```bash
    npm run dev
    ```
    The application will be available at `http://localhost:5173`.

---

## 4. Usage Examples

### Scenario 1: Generating a Code Review Prompt for a GitHub Repo
1.  **Input:** Paste a GitHub URL (e.g., `https://github.com/facebook/react`).
2.  **Template:** Select "Code Review" from the template dropdown.
3.  **Process:** Click **Generate**.
4.  **Result:** You receive a Markdown prompt containing the React file structure and core logic, which you can paste into Gemini to ask: *"Identify potential memory leaks in the hooks implementation."*

### Scenario 2: Deep Search in a Large Local Project (RAG)
1.  **Input Mode:** Select "Local Folder" and upload your project.
2.  **Enable RAG:** Toggle "Use RAG (Smart Context Filter)".
3.  **Search Query:** Enter *"How is user authentication handled?"*
4.  **Ollama Config:** Select `nomic-embed-text` for embeddings and `llama3` for processing.
5.  **Process:** The tool will find specific snippets in `auth.ts`, `passport.js`, and `middleware.tsx`, ignoring unrelated CSS or UI files.

### Scenario 3: Privacy-First Summarization
1.  **Input:** Local project folder.
2.  **Settings:** Enable "Use Local Model for Final Generation".
3.  **Settings:** Enable "Summarize source files with Ollama".
4.  **Execution:** The entire process runs on your hardware. No code is sent to Google's servers. The resulting prompt is a condensed summary of your architecture ready for any offline LLM.

---

## 5. Key Configuration Parameters

| Parameter | Description |
| :--- | :--- |
| **Max Files** | Limits the number of files processed to prevent browser crashes. |
| **Context Window** | Sets the Ollama `num_ctx` (e.g., 8192, 32768). Increase this for large files. |
| **Top-K (RAG)** | Number of relevant code segments to include in the final prompt. |
| **Temperature** | Controls "creativity" of the summary (0.1 - 0.3 recommended for code). |
| **Ollama URL** | Usually `http://127.0.0.1:11434`. |
